---
title: Testing
layout: default
output: bookdown::html_chapter
---

```{r, echo = FALSE}
library(testthat)
```

# Testing {#tests}

Testing is a vital part of package development because it ensures that your code does what you think it does. Testing adds an additional step to your development workflow. So far your workflow looks like this:

1. Write a function.
1. Load it with Cmd + Shift + L or `devtools::load_all()`.
1. Experiment with it in the console to see if it works.
1. Rinse and repeat.

You _are_ testing your code, but you're doing it informally. The problem with this approach is that when you come back to this code in 3 months time to add a new feature, you've forgotten some of the informal tests you ran the first time. That makes it very easy to break code that used to work. 

I started automating my tests because I discovered I was spending too much time recreating bugs that I had previously fixed. While writing code or fixing bugs, I'd perform interactive tests to make sure the code worked. But I never had a system which could store those tests so I could re-run them as needed. I think that this is a common practice among R programmers. It's not that you don't test your code, it's that you don't automate the tests.

The goal of this chapter is to help you graduate from informal tests at the command line to formal automated tests using the testthat package. Turning your casual interactive tests into reproducible scripts requires a little more work up front, but it pays off in four ways:

* Fewer bugs. Because you're explicit about how your code should be behave
  you will have fewer bugs, and bugs that you've fixed in the past will
  never come back to haunt you. Testing is a bit like double entry book-keeping:
  because you've described the behaviour of your code in two ways (in the code
  and in the tests), you can check them against each other.

* Better code structure. Code that's easy to test is usually better designed. I 
  have found writing tests makes me break up complicated parts of my code into 
  separate functions that can work in isolation. These functions have less 
  duplication, and are easier to test, understand and re-combine in new ways.

* Easier to pick up where you left off. If you always finish a coding session by 
  creating a failing test (e.g. for the feature you want to implement next), 
  testing makes it easier to pick up where you left off: your tests let you know 
  what to do next.

* Increased confidence when making changes. If you know that all major
  functionality has an associated test, you can confidently make big
  changes without worrying about accidentally breaking something. For me,
  this is particularly useful when I think of a simpler way to accomplish a 
  task: often my simpler solution is only simpler because I've forgotten an 
  important use case!

## Test workflow {#test-workflow}

To set up your package to use testthat, run:

```{r, eval = FALSE}
devtools::use_testthat()
```

This will:

1.  Create a `tests/testthat` directory.

1.  Add necessary dependencies to `DESCRIPTION` (i.e., it adds testthat to 
    the `Suggests` field).

1.  Creates a file `tests/testthat.R` that ensures your tests are automatically 
    run by `R CMD check`.

Once you're set up the workflow is simple:

1.  Modify your code or tests.

2.  Test your package with Cmd + Shift + T or `devtools::test()`.

3.  Repeat until all tests pass.

The testing output looks like this:

    Expectation : ...........
    rv : ...
    Variance : ....123.45.

Each line represents a file of tests. Each `.` represents a passing test. Each number represents a failing test. The numbers index into a list of failures that provides more details:

    1. Failure(@test-variance.R#22): Variance correct for discrete uniform rvs -----
    VAR(dunif(0, 10)) not equal to var_dunif(0, 10)
    Mean relative difference: 3
    
    2. Failure(@test-variance.R#23): Variance correct for discrete uniform rvs -----
    VAR(dunif(0, 100)) not equal to var_dunif(0, 100)
    Mean relative difference: 3.882353

Each failure gives a description of the test (e.g., "Variance correct for discrete uniform rvs"), a location (e.g., "\@test-variance.R#22"), and the reason for the failure (e.g., "VAR(dunif(0, 10)) not equal to var_dunif(0, 10)"). The goal is to get all the tests passing.

## Test structure

A test file lives in `tests/testthat/` and its name must start with `test`. Here's an example of a test file from the stringr package:

```{r}
library(stringr)
context("String length")

test_that("str_length is number of characters", {
  expect_equal(str_length("a"), 1)
  expect_equal(str_length("ab"), 2)
  expect_equal(str_length("abc"), 3)
})

test_that("str_length of factor is length of level", {
  expect_equal(str_length(factor("a")), 1)
  expect_equal(str_length(factor("ab")), 2)
  expect_equal(str_length(factor("abc")), 3)
})

test_that("str_length of missing is missing", {
  expect_equal(str_length(NA), NA_integer_)
  expect_equal(str_length(c(NA, 1)), c(NA, 1))
  expect_equal(str_length("NA"), 2)
})
```

Test files are made up of three components: expectations, tests and contexts. Expectations give you the tools to convert your visual, interactive experiments into reproducible scripts. Tests and contexts are ways of organising your expectations so that when something goes wrong you can easily track down the source of the problem.

* An __expectation__ is the atom of testing. It describes the expected result 
  of a computation: Does it have the right value and right class? Does it 
  produce error messages when it should? An expectation automates visual
  checking of results in the console.

* A __test__ groups together multiple expectations to fully test the output
  of one simple function, or the range of possibilities from one parameter
  of a more complicated function, or tightly related functionality across 
  multiple functions. A test is created with the `test_that()` function.

* A __context__ groups together multiple tests that test related
  functionality.  Contexts are defined with the `context()` function.

These are described in detail below. 

### Expectations

An expectation is the finest level of testing. It makes a binary assertion about whether or not a value is as you expect. All expectations have a similar structure:

* They start with `expect_`.

* They have two arguments: the first argument is the actual result, the 
  second argument is what you expect.
  
* If the actual and expected results don't agree, testthat throws an error.

While you'll normally put expectations inside tests inside contexts, you can run them directly. This makes them easy to explore interactively. There are almost 20 expectations in the testthat package. The most important are discussed below.

*   There are two basic ways to test for equality: `expect_equal()`, 
    and `expect_identical()`. `expect_equal()` is most common: it uses 
    `all.equal()` to check for equality within a numerical tolerance:

    ```{r, error = TRUE}
    expect_equal(10, 10)
    expect_equal(10, 10 + 1e-7)
    expect_equal(10, 11)
    ```
  
    If you want to test for exact equivalence, or need to compare a more
    exotic object like an environment, use `expect_identical()`. It's built
    on top of `identical()`:

    ```{r, error = TRUE}
    expect_equal(10, 10 + 1e-7)
    expect_identical(10, 10 + 1e-7)
    ```

*   `expect_match()` matches a character vector against a regular expression. The 
    optional `all` argument controls whether all elements or just one element 
    needs to match. This is powered by `grepl()`, and additional arguments, 
    like `ignore.case = FALSE` or `fixed = TRUE`, are passed on down.

    ```{r, error = TRUE}
    string <- "Testing is fun!"

    expect_match(string, "Testing") 
    # Fails, match is case-sensitive
    expect_match(string, "testing")

    # Additional arguments are passed to grepl:
    expect_match(string, "testing", ignore.case = TRUE)
    ```

*   `expect_match()` is used by three expectations that check for various 
    types of output: `expect_output()`, for printed output; `expect_message()`
    for messages; `expect_warning()` for warnings; and `expect_error()` for
    errors.
    
    ```{r, error = TRUE}
    a <- list(1:10, letters)

    expect_output(str(a), "List of 2")
    expect_output(str(a), "int [1:10]", fixed = TRUE)

    expect_message(library(mgcv), "This is mgcv")
    ```
    
    With `expect_message()`, `expect_warning()`, `expect_error()` you can
    leave the second argument blank if you just want to see if a message,
    warning or error is created. However, it's normally better to be explicit, 
    and provide some text from the message.
    
    ```{r, error = TRUE}  
    expect_warning(log(-1))
    expect_error(1 / "a") 

    # But always better to be explicit
    expect_warning(log(-1), "NaNs produced")
    expect_error(1 / "a", "non-numeric argument")

    # Failure to produce a warning or error when expected is an error
    expect_warning(log(0))
    expect_error(1 / 2) 
    ```

*   `expect_is()` checks that an object `inherit()`s from a specified class.

    ```{r, error = TRUE}
    model <- lm(mpg ~ wt, data = mtcars)
    expect_is(model, "lm")
    expect_is(model, "glm")
    ```

*   `expect_true()` and `expect_false()` are useful catchalls if none of the 
    other expectations do what need.

*   Sometimes you don't know exactly what the result should be, or it's too 
    complicated to easily recreate in code. In that case the best you can do is 
    check that the result is the same as last time. `expect_equal_to_reference()` 
    caches the result of the first test, and then compares subsequent runs to
    that. If for some reason the result does change, just delete the cache
    file and re-test.

Running a sequence of expectations is useful because it ensures that your code behaves as expected. You could even use an expectation within a function to check that the inputs are what you expect. However, they're not so useful when something goes wrong: all you know is that something is not as expected. You don't know anything about where the problem is. Tests, described next, organise expectations into coherent blocks that describe the overall goal of a set of expectations.

## Tests

Each test should test a single item of functionality and have an informative name. The idea is that when a test fails, you should know exactly where to look for the problem in your code. You create a new test with `test_that()`, with parameters name and code block. The test name should complete the sentence "Test that" and the code block should be a collection of expectations. When there's a failure, it's the test name that will help you figure out what's gone wrong.

It's up to you how to organise your expectations into tests. The main thing is that the message associated with the test should be informative so that you can quickly narrow down the source of the problem. Try to avoid putting too many expectations in one test - it's better to have more smaller tests than fewer big tests.

Each test is run in its own environment so it is self-contained. The exceptions are actions which have effects outside the local environment. These include things that affect:

* The filesystem: creating and deleting files, changing the working directory,
  etc.

* The search path: `library()`, `attach()`.

* Global options, like `options()` and `par()`.

When you use these actions in tests, you'll need to clean up after yourself. Many other testing packages have set-up and teardown methods that are run automatically before and after each test. These are not so important with testthat because you can create objects outside of the tests and rely on R's copy-on-modify semantics to keep them unchanged between test runs. To clean up other actions you can use regular R functions.

### Refactoring common behaviour

The following code shows one test of the `floor_date()` function from `library(lubridate)`. There are 7 expectations that check the results of rounding a date down to the nearest second, minute, hour, etc.  There's a lot of duplication in these tests, so we might want to extract common behaviour into a new function.

```{r}
library(lubridate)
test_that("floor_date works for different units", {
  base <- as.POSIXct("2009-08-03 12:01:59.23", tz = "UTC")

  expect_equal(floor_date(base, "second"), 
    as.POSIXct("2009-08-03 12:01:59", tz = "UTC"))
  expect_equal(floor_date(base, "minute"), 
    as.POSIXct("2009-08-03 12:01:00", tz = "UTC"))
  expect_equal(floor_date(base, "hour"),   
    as.POSIXct("2009-08-03 12:00:00", tz = "UTC"))
  expect_equal(floor_date(base, "day"),    
    as.POSIXct("2009-08-03 00:00:00", tz = "UTC"))
  expect_equal(floor_date(base, "week"),   
    as.POSIXct("2009-08-02 00:00:00", tz = "UTC"))
  expect_equal(floor_date(base, "month"),  
    as.POSIXct("2009-08-01 00:00:00", tz = "UTC"))
  expect_equal(floor_date(base, "year"),   
    as.POSIXct("2009-01-01 00:00:00", tz = "UTC"))
})
```

Note how we've defined a couple of helper functions to make the test more concise so you can easily see what changes in each expectation.

```{r}
test_that("floor_date works for different units", {
  base <- as.POSIXct("2009-08-03 12:01:59.23", tz = "UTC")
  floor_base <- function(unit) floor_date(base, unit)
  as_time <- function(x) as.POSIXct(x, tz = "UTC")

  expect_equal(floor_base("second"), as_time("2009-08-03 12:01:59"))
  expect_equal(floor_base("minute"), as_time("2009-08-03 12:01:00"))
  expect_equal(floor_base("hour"),   as_time("2009-08-03 12:00:00"))
  expect_equal(floor_base("day"),    as_time("2009-08-03 00:00:00"))
  expect_equal(floor_base("week"),   as_time("2009-08-02 00:00:00"))
  expect_equal(floor_base("month"),  as_time("2009-08-01 00:00:00"))
  expect_equal(floor_base("year"),   as_time("2009-01-01 00:00:00"))
})
```

We could go a step further and create a custom expectation function:

```{r}
base <- as.POSIXct("2009-08-03 12:01:59.23", tz = "UTC")

expect_floor_equal <- function(unit, time) {
  expect_equal(floor_date(base, unit), as.POSIXct(time, tz = "UTC"))
}
expect_floor_equal("year", "2009-01-01 00:00:00")
```

However, this doesn't give very informative output if the expectation fails:

```{r, error = TRUE}
expect_floor_equal("year", "2008-01-01 00:00:00")
```

Instead you can use a little [non-standard evaluation](http://adv-r.had.co.nz/Computing-on-the-language.html) to produce more informative output. The key is to use `bquote()` and `eval()`. In the `bquote()` call below, note the use of `.(x)` - the value of anything inside will be inserted into the call.

```{r, error = TRUE}
expect_floor_equal <- function(unit, time) {
  as_time <- function(x) as.POSIXct(x, tz = "UTC")
  eval(bquote(expect_equal(floor_date(base, .(unit)), as_time(.(time)))))
}
expect_floor_equal("year", "2008-01-01 00:00:00")
```

This sort of refactoring is often worthwhile because it removing repeated code makes it easier to see what's changing. It's important that your tests be easy to read so that you can be confident they're correct.

```{r}
test_that("floor_date works for different units", {
  as_time <- function(x) as.POSIXct(x, tz = "UTC")
  expect_floor_equal <- function(unit, time) {
    eval(bquote(expect_equal(floor_date(base, .(unit)), as_time(.(time)))))
  }

  base <- as_time("2009-08-03 12:01:59.23")
  expect_floor_equal("second", "2009-08-03 12:01:59")
  expect_floor_equal("minute", "2009-08-03 12:01:00")
  expect_floor_equal("hour",   "2009-08-03 12:00:00")
  expect_floor_equal("day",    "2009-08-03 00:00:00")
  expect_floor_equal("week",   "2009-08-02 00:00:00")
  expect_floor_equal("month",  "2009-08-01 00:00:00")
  expect_floor_equal("year",   "2009-01-01 00:00:00")
})
```

### Skipping a test

Sometimes it's impossible to perform a test.

## Contexts

Contexts group tests together into blocks that test related functionality, and are established with the code `context("My context")`. Normally there is one context per file, but you can have more if you want, or you can use the same context in multiple files.

The following code shows the context that tests the operation of the `stringr::str_length()`. The tests are very simple. They cover two situations where `nchar()` from base R gives surprising results.


## Confirmatory programming

Confirmatory programming happens when you know what you need to do and what the results of your changes will be (new feature X appears or known bug Y disappears); you just need to figure out the way to do it. Confirmatory programming is also known as [test driven development][tdd] (TDD), a development style that grew out of [extreme programming](extreme-programming). The basic idea is that, before you implement any new feature or fix a known bug, you should:

1. Write an automated test and run `test()` to make sure the test fails (so you know
   you've captured the bug correctly).

2. Modify code to fix the bug or implement the new feature.

3. Run `test(pkg)` to reload the package and re-run the tests.

4. Repeat 2--3 until all tests pass.

5. Update documentation comments, run `document()`, and update `NEWS`.

For this paradigm, you might also want to use `testthat::auto_test()`, which will watch your tests and code and will automatically rerun your tests when either changes. This allows you to skip step three: you just modify your code and watch to see if the tests pass or fail.

## CRAN notes

* Tests need to run relatively quickly. Place `skip_on_cran()` at the beginning
  of long-running tests that shouldn't be run on CRAN.

[tdd]:http://en.wikipedia.org/wiki/Test-driven_development
[extreme-programming]:http://en.wikipedia.org/wiki/Extreme_programming
