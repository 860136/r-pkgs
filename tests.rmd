---
title: Testing
layout: default
output: bookdown::html_chapter
---

```{r, echo = FALSE}
library(testthat)
```

# Testing {#tests}

To make sure that your package behaves as you'd expect, testing is vital. While you probably test your code already, you may not have taken the next step: automation. This chapter describes how to use testthat to create automated tests of your code.

I started automating my tests because I discovered I was spending too much time recreating bugs that I had previously fixed. While writing code or fixing bugs, I'd perform interactive tests to make sure the code worked. But I never had a system which could store those test so I could re-run them as needed. I think that this is a common practice among R programmers. It's not that we don't test our code, it's that we don't have a way to make it easy to re-run tests, let alone to do so automatically.

Turning your casual interactive tests into reproducible scripts requires a little more work up front. Since you can no longer visually inspect the output, you have to write code that does the inspection for you. However, this is an investment in the future of your code. It pays off in four ways:

* Decreased frustration. Whenever I'm on a strict deadline I always seem to 
  discover a bug in old code. Having to stop what I'm doing to fix it is a real 
  pain. The more I test, the less this happens. Also, based on how well they 
  test, I can easily see which parts of my code I can be confident about.

* Better code structure. Code that's easy to test is usually better designed. I 
  have found writing tests makes me break up complicated parts of my code into 
  separate functions that can work in isolation. These functions have less 
  duplication, and are easier to test, understand and re-combine.

* Easier to pick up where you left off. If you always finish a coding session by 
  creating a failing test (e.g. for the feature you want to implement next), 
  testing makes it easier to pick up where you left off: your tests let you know 
  what to do next.

* Increased confidence when making changes. If you know that all major
  functionality has an associated test, you can confidently make big
  changes without worrying about accidentally breaking something. For me,
  this is particularly useful when I think of a simpler way to accomplish a 
  task: often my simpler solution is only simpler because I've forgotten an 
  important use case!

## testthat test structure

`testthat` has a hierarchical structure made up of expectations, tests and contexts. 

* An __expectation__ describes the expected result of a computation: Does it
  have the right value and right class? Does it produce error messages when it
  should? There are 11 types of built in expectations.

* A __test__ groups together multiple expectations to test one function, or
  tightly related functionality across multiple functions. A test is created
  with the `test_that()` function.

* A __context__ groups together multiple tests that test related
  functionality.  Contexts are defined with the `context()` function.

These are described in detail below. 

Expectations give you the tools to convert your visual, interactive experiments into reproducible scripts. Tests and contexts are ways of organising your expectations so that when something goes wrong you can easily track down the source of the problem.

### Expectations

An expectation is the finest level of testing. It makes a binary assertion about whether or not a value is as you expect. If the expectation isn't true, `testthat` will raise an error.

An expectation is easy to read. Its syntax is close to a sentence in English: `expect_equal(a, b)` reads as "I expect that a and b are equal".

There are 11 built in expectations:

*   `expect_equal()` uses `all.equal()` to check for equality within some 
    numerical tolerance:

    ```{r, error = TRUE}
    expect_equal(10, 10)
    expect_equal(10, 10 + 1e-7)
    expect_equal(10, 10 + 1e-6)
    expect_equal(10, 11) 
    ```
  
*   `expect_identical()` uses `identical()` to check for exact equality.

    ```{r, error = TRUE}
    expect_identical(10, 10) 
    expect_identical(10, 10 + 1e-7)
    ```

*   `is_equivalent_to()` is a more relaxed version of `equals()` that ignores
    attributes:

    ```{r, error = TRUE}
    expect_equal(c("one" = 1, "two" = 2), 1:2)
    expect_equivalent(c("one" = 1, "two" = 2), 1:2)
    ```

*   `expect_is()` checks that an object `inherit()`s from a specified class.

    ```{r, error = TRUE}
    model <- lm(mpg ~ wt, data = mtcars)
    expect_is(model, "lm")
    expect_is(model, "glm")
    ```

*   `expect_match()` matches a character vector against a regular expression. The 
    optional `all` argument controls whether all elements or just one element 
    needs to match. This is powered by `grepl()`.

    ```{r, error = TRUE}
    string <- "Testing is fun!"

    expect_match(string, "Testing") 
    # Fails, match is case-sensitive
    expect_match(string, "testing")

    # Additional arguments are passed to grepl:
    expect_match(string, "testing", ignore.case = TRUE)
    ```

*   `expect_output()` matches the printed output from an expression against a
    regular expression.

    ```{r, error = TRUE}
    a <- list(1:10, letters)

    expect_output(str(a), "List of 2")
    expect_output(str(a), "int [1:10]", fixed = TRUE)
    ```

*   `expect_message()` checks that an expression shows a message:

    ```{r, error = TRUE}
    expect_message(library(mgcv), "This is mgcv")
    ```

*   `expect_warning()` expects that you get a warning.

    ```{r, error = TRUE}
    expect_warning(log(-1))
    # But always better to be explicit
    expect_warning(log(-1), "NaNs produced")

    expect_warning(log(0))
    ```

*   `expect_error()` verifies that the expression throws an error. You can also
    supply a regular expression which is applied to the text of the error.

    ```{r, error = TRUE}
    expect_error(1 / 2) 
    expect_error(1 / "a") 
    # Better to be explicit
    expect_error(1 / "a", "non-numeric argument")
    ```

*   `expect_true()` is a useful catchall if none of the other expectations do what
    you want - it checks that an expression is true. `expect_false()` is the
    complement of `expect_true()`.

Running a sequence of expectations is useful because it ensures that your code behaves as expected. You could even use an expectation within a function to check that the inputs are what you expect. However, they're not so useful when something goes wrong: all you know is that something is not as expected. You don't know anything about where the problem is. Tests, described next, organise expectations into coherent blocks that describe the overall goal of a set of expectations.

### Tests

Each test should test a single item of functionality and have an informative name. The idea is that when a test fails, you should know exactly where to look for the problem in your code. You create a new test with `test_that()`, with parameters name and code block. The test name should complete the sentence "Test that" and the code block should be a collection of expectations. When there's a failure, it's the test name that will help you figure out what's gone wrong.

The following code shows one test of the `floor_date()` function from `library(lubridate)`. There are 7 expectations that check the results of rounding a date down to the nearest second, minute, hour, etc. Note how we've defined a couple of helper functions to make the test more concise so you can easily see what changes in each expectation.

```{r, eval = FALSE}
test_that("floor_date works for different units", {
  base <- as.POSIXct("2009-08-03 12:01:59.23", tz = "UTC")

  expect_time <- function(x, y) expect_equal(x, as.POSIXct(y, tz = "UTC"))
  floor_base <- function(unit) floor_date(base, unit)

  expect_time(floor_base("second"), "2009-08-03 12:01:59")
  expect_time(floor_base("minute"), "2009-08-03 12:01:00")
  expect_time(floor_base("hour"),   "2009-08-03 12:00:00")
  expect_time(floor_base("day"),    "2009-08-03 00:00:00")
  expect_time(floor_base("week"),   "2009-08-02 00:00:00")
  expect_time(floor_base("month"),  "2009-08-01 00:00:00")
  expect_time(floor_base("year"),   "2009-01-01 00:00:00")
})
```

Each test is run in its own environment so it is self-contained. The exceptions are actions which have effects outside the local environment. These include things that affect:

* The filesystem: creating and deleting files, changing the working directory,
  etc.

* The search path: `library()`, `attach()`.

* Global options, like `options()` and `par()`.

When you use these actions in tests, you'll need to clean up after yourself. Many other testing packages have set-up and teardown methods that are run automatically before and after each test. These are not so important with `testthat` because you can create objects outside of the tests and rely on R's copy-on-modify semantics to keep them unchanged between test runs. To clean up other actions you can use regular R functions.

## Contexts

Contexts group tests together into blocks that test related functionality, and are established with the code `context("My context")`. Normally there is one context per file, but you can have more if you want, or you can use the same context in multiple files.

The following code shows the context that tests the operation of the `str_length()` function in `stringr`. The tests are very simple. They cover two situations where `nchar()` from base R gives surprising results.

```{r, eval = FALSE}
context("String length")

test_that("str_length is number of characters", {
  expect_equal(str_length("a"), 1)
  expect_equal(str_length("ab"), 2)
  expect_equal(str_length("abc"), 3)
})

test_that("str_length of factor is length of level", {
  expect_equal(str_length(factor("a")), 1)
  expect_equal(str_length(factor("ab")), 2)
  expect_equal(str_length(factor("abc")), 3)
})

test_that("str_length of missing is missing", {
  expect_equal(str_length(NA), NA_integer_)
  expect_equal(str_length(c(NA, 1)), c(NA, 1))
  expect_equal(str_length("NA"), 2)
})
```

## R CMD check

If you need to do two things to make sure `R CMD check` will run your tests:

1.  Add testthat to `Suggests` field in `DESCRIPTION`.

1.  Create `tests/testthat.R` that contains:

    ```{r, eval = FALSE}
    library(testthat)
    test_check("yourpackage")
    ```

## Development cycles

It's useful to distinguish between exploratory programming and confirmatory programming (in the same sense as exploratory and confirmatory data analysis), because the development cycle differs in several important ways.

### Confirmatory programming

Confirmatory programming happens when you know what you need to do and what the results of your changes will be (new feature X appears or known bug Y disappears); you just need to figure out the way to do it. Confirmatory programming is also known as [test driven development][tdd] (TDD), a development style that grew out of [extreme programming](extreme-programming). The basic idea is that, before you implement any new feature or fix a known bug, you should:

1. Write an automated test and run `test()` to make sure the test fails (so you know
   you've captured the bug correctly).

2. Modify code to fix the bug or implement the new feature.

3. Run `test(pkg)` to reload the package and re-run the tests.

4. Repeat 2--3 until all tests pass.

5. Update documentation comments, run `document()`, and update `NEWS`.

For this paradigm, you might also want to use `testthat::auto_test()`, which will watch your tests and code and will automatically rerun your tests when either changes. This allows you to skip step three: you just modify your code and watch to see if the tests pass or fail.

### Exploratory programming

Exploratory programming is the complement of confirmatory programming, when you have some idea of what you want to achieve, but you're not sure about the details. You're not sure what the functions should look like, what arguments they should have and what they should return. You may not even be sure how you are going to break down the problem into pieces. In exploratory programming, you're exploring the solution space by writing functions and you need the freedom to rewrite large chunks of the code as you understand the problem domain better.

The exploratory programming cycle is similar to confirmatory, but it's not usually worth writing the tests before writing the code, because the interface will change so much:

1. Edit code and reload with `load_all()`.

2. Test interactively.

3. Repeat 1--2 until code works.

4. Write automated tests and `test()`.

5. Update documentation comments, run `document()`, and update `NEWS`

The automated tests are still vitally important because they are what will prevent your code from failing silently in the future.

[tdd]:http://en.wikipedia.org/wiki/Test-driven_development
[extreme-programming]:http://en.wikipedia.org/wiki/Extreme_programming
